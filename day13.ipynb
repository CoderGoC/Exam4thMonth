{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.6.4)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from category_encoders) (2.0.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from category_encoders) (1.5.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from category_encoders) (1.14.0)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from category_encoders) (0.14.4)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from category_encoders) (2.2.2)\n",
      "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from category_encoders) (0.5.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
      "Requirement already satisfied: six in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.20.0->category_encoders) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.20.0->category_encoders) (3.5.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from statsmodels>=0.9.0->category_encoders) (24.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install category_encoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import necessary libraries\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "# from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "# from sklearn.ensemble import StackingClassifier\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from category_encoders.woe import WOEEncoder  # For WoE encoding\n",
    "# from sklearn.feature_selection import mutual_info_classif\n",
    "# import phik\n",
    "# from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "# import numpy as np\n",
    "\n",
    "# # Load your train and test datasets\n",
    "# train_df = pd.read_csv('train.csv')\n",
    "# test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# # Separate target variable from features\n",
    "# y = train_df['Exited']\n",
    "# X = train_df.drop(columns=['Exited', 'id'])  # Drop 'id' and target 'Exited' from the training features\n",
    "# X_test = test_df.drop(columns=['id'])  # Drop 'id' from the test features\n",
    "\n",
    "# # Step 1: Divide the train data into two parts (train/validation split)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Step 2: Feature Engineering on Each Column Separately\n",
    "\n",
    "# # List of columns to be processed\n",
    "# categorical_columns = ['Geography', 'Gender']  # Example categorical columns\n",
    "# numerical_columns = ['CreditScore', 'Age', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "\n",
    "# # WoE Encoding for categorical columns\n",
    "# woe_encoder = WOEEncoder(cols=categorical_columns)\n",
    "\n",
    "# # SimpleImputer for missing values and StandardScaler for numerical columns\n",
    "# numerical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='median')),  # Impute missing values with median\n",
    "#     ('scaler', StandardScaler())  # Standard scaling for numerical columns\n",
    "# ])\n",
    "\n",
    "# # Apply WoE encoding to categorical columns\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('woe', woe_encoder)\n",
    "# ])\n",
    "\n",
    "# # Combine preprocessing steps using ColumnTransformer\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numerical_transformer, numerical_columns),\n",
    "#         ('cat', categorical_transformer, categorical_columns)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Step 3: Model Selection and Training\n",
    "\n",
    "# # Define base models for stacking\n",
    "# log_reg = LogisticRegression(solver='liblinear', max_iter=1000)\n",
    "# ridge_clf = RidgeClassifier()\n",
    "\n",
    "# # Combine models using stacking\n",
    "# base_models = [\n",
    "#     ('ridge', ridge_clf),\n",
    "#     ('log_reg', log_reg)\n",
    "# ]\n",
    "\n",
    "# stack_model = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression(solver='liblinear', max_iter=1000))\n",
    "\n",
    "# # Create a pipeline with preprocessing and the model\n",
    "# pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('stack_model', stack_model)])\n",
    "\n",
    "# # Step 4: Train the Model and Evaluate on the Validation Set\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on validation set\n",
    "# y_val_pred = pipeline.predict(X_val)\n",
    "# val_roc_auc = roc_auc_score(y_val, y_val_pred)\n",
    "# val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "# print(f'Validation ROC-AUC: {val_roc_auc}')\n",
    "# print(f'Validation Accuracy: {val_accuracy}')\n",
    "\n",
    "# # Step 5: Hyperparameter Tuning for Improvement\n",
    "# param_grid = {\n",
    "#     'stack_model__final_estimator__C': [0.1, 1, 10, 100],\n",
    "#     'stack_model__final_estimator__penalty': ['l1', 'l2']\n",
    "# }\n",
    "\n",
    "# # Use GridSearchCV to find optimal hyperparameters\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc')\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Best model from grid search\n",
    "# best_model = grid_search.best_estimator_\n",
    "# print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# # Predict with the best model\n",
    "# y_val_best_pred = best_model.predict(X_val)\n",
    "# best_val_roc_auc = roc_auc_score(y_val, y_val_best_pred)\n",
    "# best_val_accuracy = accuracy_score(y_val, y_val_best_pred)\n",
    "\n",
    "# print(f'Best Validation ROC-AUC: {best_val_roc_auc}')\n",
    "# print(f'Best Validation Accuracy: {best_val_accuracy}')\n",
    "\n",
    "# # Step 6: Predictions on Test Data with Best Model\n",
    "# y_test_pred = best_model.predict_proba(X_test)[:, 1]  # Probability for class \"Exited\"\n",
    "\n",
    "# # Prepare the submission file\n",
    "# submission = test_df[['id']].copy()\n",
    "# submission['Exited'] = y_test_pred\n",
    "# submission.to_csv('Final.csv', index=False)\n",
    "\n",
    "# print(\"Final file created successfully.\")\n",
    "\n",
    "# # Step 7: Feature Importance - Mutual Information (Optional for feature selection)\n",
    "# X_train_transformed = preprocessor.fit_transform(X_train, y_train)  # Apply transformation for feature selection\n",
    "# mi_scores = mutual_info_classif(X_train_transformed, y_train)\n",
    "# mi_scores_series = pd.Series(mi_scores, index=numerical_columns + categorical_columns)\n",
    "# print(f\"Mutual Information Scores:\\n{mi_scores_series.sort_values(ascending=False)}\")\n",
    "\n",
    "# # Step 8: PhiK Correlation (Optional for non-linear relationships)\n",
    "# phik_matrix = pd.DataFrame(X_train).phik_matrix(interval_cols=numerical_columns)\n",
    "# print(f\"PhiK Correlation Matrix:\\n{phik_matrix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional, got ndarray of shape (3, 2) instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# 3.2 Group Statistics (Example: Mean balance per Geography group)\u001b[39;00m\n\u001b[0;32m     55\u001b[0m geography_balance_mean \u001b[38;5;241m=\u001b[39m X_train_fe\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeography\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBalance\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m---> 56\u001b[0m X_train_fe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeography_Balance_Mean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mX_train_fe\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGeography\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeography_balance_mean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m X_val_fe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeography_Balance_Mean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X_val_fe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeography\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(geography_balance_mean)\n\u001b[0;32m     58\u001b[0m X_test_fe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeography_Balance_Mean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X_test_fe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeography\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(geography_balance_mean)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4700\u001b[0m, in \u001b[0;36mSeries.map\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   4620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\n\u001b[0;32m   4621\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4622\u001b[0m     arg: Callable \u001b[38;5;241m|\u001b[39m Mapping \u001b[38;5;241m|\u001b[39m Series,\n\u001b[0;32m   4623\u001b[0m     na_action: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   4624\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[0;32m   4625\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4626\u001b[0m \u001b[38;5;124;03m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[0;32m   4627\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4698\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[0;32m   4699\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4700\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_values, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4702\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4703\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\algorithms.py:1724\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1722\u001b[0m             mapper \u001b[38;5;241m=\u001b[39m Series(mapper, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m   1723\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1724\u001b[0m             mapper \u001b[38;5;241m=\u001b[39m \u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1726\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapper, ABCSeries):\n\u001b[0;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:584\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    582\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    586\u001b[0m     manager \u001b[38;5;241m=\u001b[39m _get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\construction.py:633\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m         data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    642\u001b[0m     _sanitize_non_ordered(data)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\construction.py:659\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[0;32m    656\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, subarr)\n\u001b[0;32m    657\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m maybe_infer_to_datetimelike(subarr)\n\u001b[1;32m--> 659\u001b[0m subarr \u001b[38;5;241m=\u001b[39m \u001b[43m_sanitize_ndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_2d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(subarr, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    662\u001b[0m     \u001b[38;5;66;03m# at this point we should have dtype be None or subarr.dtype == dtype\u001b[39;00m\n\u001b[0;32m    663\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mdtype, dtype)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\construction.py:718\u001b[0m, in \u001b[0;36m_sanitize_ndim\u001b[1;34m(result, data, dtype, index, allow_2d)\u001b[0m\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allow_2d:\n\u001b[0;32m    717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m--> 718\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    719\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData must be 1-dimensional, got ndarray of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    720\u001b[0m     )\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;66;03m# i.e. NumpyEADtype(\"O\")\u001b[39;00m\n\u001b[0;32m    724\u001b[0m     result \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mValueError\u001b[0m: Data must be 1-dimensional, got ndarray of shape (3, 2) instead"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from category_encoders.woe import WOEEncoder\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Separate target variable from features\n",
    "y = train_df['Exited']\n",
    "X = train_df.drop(columns=['Exited', 'id'])  # Drop 'id' and target 'Exited' from the training features\n",
    "X_test = test_df.drop(columns=['id'])  # Drop 'id' from the test features\n",
    "\n",
    "# List of categorical and numerical columns\n",
    "categorical_columns = ['Geography', 'Gender']  # Categorical columns\n",
    "numerical_columns = ['CreditScore', 'Age', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "\n",
    "# Step 1: Apply WoE encoding to categorical features\n",
    "woe_encoder = WOEEncoder(cols=categorical_columns)\n",
    "X_train_woe = woe_encoder.fit_transform(X, y)  # WoE encode the train data\n",
    "X_test_woe = woe_encoder.transform(X_test)  # WoE encode the test data\n",
    "\n",
    "# Step 2: Split the encoded data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_woe, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Step 3: Feature Engineering on Each Column Separately\n",
    "\n",
    "# Create a copy of the dataset to perform feature engineering\n",
    "X_train_fe = X_train.copy()\n",
    "X_val_fe = X_val.copy()\n",
    "X_test_fe = X_test_woe.copy()\n",
    "\n",
    "# 3.1 Polynomial Features (Second-degree interactions)\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "poly_features_train = poly.fit_transform(X_train_fe[numerical_columns])\n",
    "poly_features_val = poly.transform(X_val_fe[numerical_columns])\n",
    "poly_features_test = poly.transform(X_test_fe[numerical_columns])\n",
    "\n",
    "# Add polynomial features to the original dataset\n",
    "poly_feature_names = poly.get_feature_names_out(numerical_columns)\n",
    "X_train_fe = pd.concat([X_train_fe, pd.DataFrame(poly_features_train, columns=poly_feature_names)], axis=1)\n",
    "X_val_fe = pd.concat([X_val_fe, pd.DataFrame(poly_features_val, columns=poly_feature_names)], axis=1)\n",
    "X_test_fe = pd.concat([X_test_fe, pd.DataFrame(poly_features_test, columns=poly_feature_names)], axis=1)\n",
    "\n",
    "# 3.2 Group Statistics (Example: Mean balance per Geography group)\n",
    "geography_balance_mean = X_train_fe.groupby('Geography')['Balance'].mean()\n",
    "X_train_fe['Geography_Balance_Mean'] = X_train_fe['Geography'].map(geography_balance_mean)\n",
    "X_val_fe['Geography_Balance_Mean'] = X_val_fe['Geography'].map(geography_balance_mean)\n",
    "X_test_fe['Geography_Balance_Mean'] = X_test_fe['Geography'].map(geography_balance_mean)\n",
    "\n",
    "# Additional feature engineering on Balance and EstimatedSalary (e.g., creating groups)\n",
    "balance_bins = [-np.inf, 0, 100000, 200000, np.inf]\n",
    "salary_bins = [-np.inf, 50000, 100000, 150000, np.inf]\n",
    "X_train_fe['Balance_Group'] = pd.cut(X_train_fe['Balance'], bins=balance_bins, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "X_val_fe['Balance_Group'] = pd.cut(X_val_fe['Balance'], bins=balance_bins, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "X_test_fe['Balance_Group'] = pd.cut(X_test_fe['Balance'], bins=balance_bins, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "X_train_fe['Salary_Group'] = pd.cut(X_train_fe['EstimatedSalary'], bins=salary_bins, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "X_val_fe['Salary_Group'] = pd.cut(X_val_fe['EstimatedSalary'], bins=salary_bins, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "X_test_fe['Salary_Group'] = pd.cut(X_test_fe['EstimatedSalary'], bins=salary_bins, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "# Step 4: Preprocessing and Model Training\n",
    "\n",
    "# Update the list of numerical columns with the new features\n",
    "numerical_columns += poly_feature_names.tolist() + ['Geography_Balance_Mean']\n",
    "\n",
    "# WoE Encoding for categorical columns\n",
    "woe_encoder = WOEEncoder(cols=categorical_columns)\n",
    "\n",
    "# SimpleImputer for missing values and StandardScaler for numerical columns\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Impute missing values with median\n",
    "    ('scaler', StandardScaler())  # Standard scaling for numerical columns\n",
    "])\n",
    "\n",
    "# Apply WoE encoding to categorical columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('woe', woe_encoder)\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define base models for stacking\n",
    "log_reg = LogisticRegression(solver='liblinear', max_iter=1000)\n",
    "ridge_clf = RidgeClassifier()\n",
    "rf_clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "# Combine models using stacking\n",
    "base_models = [\n",
    "    ('ridge', ridge_clf),\n",
    "    ('log_reg', log_reg),\n",
    "    ('rf', rf_clf),\n",
    "    ('gb', gb_clf)\n",
    "]\n",
    "\n",
    "stack_model = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression(solver='liblinear', max_iter=1000))\n",
    "\n",
    "# Create a pipeline with preprocessing and the model\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('stack_model', stack_model)])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train_fe, y_train)\n",
    "\n",
    "# Step 5: Evaluate on validation set and make predictions on test set\n",
    "y_val_pred = pipeline.predict(X_val_fe)\n",
    "val_roc_auc = roc_auc_score(y_val, y_val_pred)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "# Output validation scores\n",
    "print(f'Validation ROC-AUC: {val_roc_auc}')\n",
    "print(f'Validation Accuracy: {val_accuracy}')\n",
    "\n",
    "# Step 6: Predictions on test data with custom rounding logic for Exited values\n",
    "y_test_pred = pipeline.predict_proba(X_test_fe)[:, 1]  # Probability for class \"Exited\"\n",
    "\n",
    "# Apply custom rounding logic\n",
    "y_test_pred_custom = []\n",
    "for pred in y_test_pred:\n",
    "    if pred >= 0.75:\n",
    "        y_test_pred_custom.append(1)\n",
    "    elif 0.5 <= pred < 0.75:\n",
    "        y_test_pred_custom.append(np.round(pred, 2))  # Round to 2 decimal places if between 0.5 and 0.74\n",
    "    else:\n",
    "        y_test_pred_custom.append(pred)  # Keep the rest unchanged\n",
    "\n",
    "# Convert the list back to a NumPy array\n",
    "y_test_pred_custom = np.array(y_test_pred_custom)\n",
    "\n",
    "# Step 7: Prepare the submission file\n",
    "submission = test_df[['id']].copy()\n",
    "submission['Exited'] = y_test_pred_custom  # Add the custom-rounded predicted probabilities\n",
    "submission.to_csv('SubmissionFinal.csv', index=False)\n",
    "\n",
    "# Notify that the file was successfully created\n",
    "print(\"SubmissionFinal.csv file created successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
